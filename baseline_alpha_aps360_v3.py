# -*- coding: utf-8 -*-
"""baseline_alpha_aps360_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dZDBgXaHJHvyaJxnGiFgvSHDxgbRx4vE
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torchvision.transforms import Resize
from PIL import Image
from skimage.color import rgb2lab
import os
import torch.utils.data as data
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import PIL, urllib
from torchvision import utils
from torch.utils.data import Subset

from google.colab import drive
drive.mount('/content/drive')

class ImageLabDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = [os.path.join(root, f) for root, _, files in os.walk(root_dir) for f in files if f.lower().endswith('.jpg')]
        self.resize_transform = Resize((224, 224), antialias=True)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path)

        if self.transform:
            img = self.transform(img)

        img_array = np.array(img)
        img_lab = rgb2lab(img_array)
        L = img_lab[:, :, 0]
        AB = img_lab[:, :, 1:] / 128.0

        L = torch.tensor(L, dtype=torch.float32).unsqueeze(0)
        AB = torch.tensor(AB, dtype=torch.float32)

        # Resize both L and AB channels using the same transform
        L_resized = self.resize_transform(L.unsqueeze(0)).squeeze(0)
        AB_resized = self.resize_transform(AB.permute(2, 0, 1))

        return L_resized, AB_resized

import matplotlib.pyplot as plt
import numpy as np
google_drive_path = '/content/drive/MyDrive/APS360_Project_Data/landscape_Images/colour'
dataset = ImageLabDataset(google_drive_path)

def plot_lab_channels(L_channel, AB_channel, original_image, title):
    plt.figure(figsize=(20, 6))

    plt.subplot(2, 7, 1)
    plt.imshow(original_image)
    plt.title(f'{title} - Original Image')
    plt.axis('off')

    plt.subplot(2, 7, 2)
    plt.imshow(L_channel.squeeze(), cmap='gray')
    plt.title(f'{title} - L Channel')
    plt.axis('off')

    A_channel = AB_channel[0]
    B_channel = AB_channel[1]

    A_channel = (A_channel + 1) / 2
    B_channel = (B_channel + 1) / 2

    plt.subplot(2, 7, 3)
    plt.imshow(A_channel)
    plt.title(f'{title} - A Channel')
    plt.axis('off')

    plt.subplot(2, 7, 4)
    plt.imshow(B_channel)
    plt.title(f'{title} - B Channel')
    plt.axis('off')

    plt.show()

for idx in range(3):
    L_channel, AB_channel = dataset[idx]
    color_image = Image.open(dataset.image_paths[idx]).convert("RGB")
    plot_lab_channels(L_channel, AB_channel, color_image, f'Image {idx+1}')

# Define model
class ColorizationModel(nn.Module):
    def __init__(self):
        super(ColorizationModel, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2),
            nn.ReLU(),
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 2, kernel_size=3, padding=1),
            nn.Tanh(),
            nn.Upsample(scale_factor=2)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

def train(model, train_loader, val_loader, num_epochs=5, learning_rate=1e-4):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    train_losses, val_losses = [], []

    for epoch in range(num_epochs):
        model.train()  # Set the model to training mode
        running_train_loss = 0.0

        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            running_train_loss += loss.item() * len(inputs)

        epoch_train_loss = running_train_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        model.eval()  # Set the model to evaluation mode
        running_val_loss = 0.0

        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                running_val_loss += loss.item() * len(inputs)

        epoch_val_loss = running_val_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)

    # Plotting
    plt.figure(figsize=(12, 5))
    plt.plot(train_losses, label='Training loss')
    plt.plot(val_losses, label='Validation loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    print(f"Final Training Loss: {train_losses[-1]:.4f}")
    print(f"Final Validation Loss: {val_losses[-1]:.4f}")

from torch.utils.data import Subset

dataset = ImageLabDataset(google_drive_path)

subset = Subset(dataset, range(50))
subset_1 = Subset(dataset, range(51, 101))

train_loader = DataLoader(subset, batch_size=32, shuffle=True)
val_loader = DataLoader(subset_1, batch_size=32, shuffle=True)

# Define model
model = ColorizationModel()

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate if needed

# Train the model
train(model, train_loader, val_loader, num_epochs = 15, learning_rate=0.001)

# Save model
torch.save(model.state_dict(), 'colorization_model.pth')

# Load and preprocess the grayscale image
img_path = '/content/drive/MyDrive/APS360_Project_Data/landscape_Images/gray/110.jpg'
img_path_colour = '/content/drive/MyDrive/APS360_Project_Data/landscape_Images/colour/110.jpg'
img1_color = []
img1 = img_to_array(load_img(img_path))
img1 = resize(img1, (256, 256))
img1_color.append(img1)

img1_color = np.array(img1_color, dtype=float)
img1_color = rgb2lab(1.0/255*img1_color)[:, :, :, 0]
img1_color = img1_color.reshape(img1_color.shape + (1,))

# Predict the colorized output
output1 = model.predict(img1_color)
output1 = output1 * 128

# Create the result image
result = np.zeros((256, 256, 3))
result[:, :, 0] = img1_color[0][:, :, 0]
result[:, :, 1:] = output1[0]

# Display the images side by side
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(np.clip(lab2rgb(result), 0, 1))
axes[0].set_title('Modelled Image')
axes[0].axis('off')

img_array_grey = img_to_array(load_img(img_path))
axes[1].imshow(np.clip(img_array_grey.squeeze(), 0, 255).astype(np.uint8))
axes[1].set_title('Input Image')
axes[1].axis('off')

img_array_colour = img_to_array(load_img(img_path_colour))
axes[2].imshow(np.clip(img_array_colour.squeeze(), 0, 255).astype(np.uint8))
axes[2].set_title('Coloured Image')
axes[2].axis('off')

plt.show()